{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ab2b1-8a86-408b-87f1-a2966c3c4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "jar_files = [\n",
    "    \"/util/clickhouse-jdbc-0.3.2-all.jar\"\n",
    "]\n",
    "\n",
    "# Initialize Spark session with JARs\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test-jdbc\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \",\".join(jar_files)) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.extensions\",\"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\",\"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\",\"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\",\"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "url = \"jdbc:ch://clickhouse:8123/cameraaberta\"\n",
    "user = \"admin\" \n",
    "password = \"admin\"  \n",
    "driver = \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d929a-815d-4497-a9ce-31ece0680076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef70267a-2801-499d-8e22-356627b8c306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d702b-55f9-41bb-b1a4-86a1009500a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stops\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", \"true\") \\\n",
    "    .csv(f\"s3a://raw/stops.txt\")\n",
    "# show data\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView('stops')\n",
    "\n",
    "spark.sql('select * from stops limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bf7c0-8641-40ea-9b0a-925d5c34470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trips\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", \"true\") \\\n",
    "    .csv(f\"s3a://raw/trips.txt\")\n",
    "# show data\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f2792-a7ca-4aef-a45f-aa795b4d8820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop-times\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", \"true\") \\\n",
    "    .csv(f\"s3a://raw/stop_times.txt\")\n",
    "# show data\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866cbea-0856-4eaa-9ac7-d7779e579dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapes\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", \"true\") \\\n",
    "    .csv(f\"s3a://raw/shapes.txt\")\n",
    "# show data\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9600e-b233-405f-86f2-90468b1270ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#routes\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", \"true\") \\\n",
    "    .csv(f\"s3a://raw/routes.txt\")\n",
    "# show data\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db85d9b-7667-425a-a817-2fd1228c0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequencies\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", \"true\") \\\n",
    "    .csv(f\"s3a://raw/frequencies.txt\")\n",
    "# show data\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1c8ca-8c1d-493f-a01d-8b509e5fe6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.option(\"inferSchema\", True) \\\n",
    "    .json(f\"s3a://raw/linhas_from_api.json\")\n",
    "df.show(10)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ffaf0-a2e1-4f22-a9aa-0949c347597d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac3e22-5a1b-46b0-ab96-aeb35e678825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd1c1f-cec9-4458-b040-c0512557dcf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8b2de-d543-4a91-82f8-b9b6f101435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ac5d7-a566-4af9-9d35-1898f8f60ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dabfc4-7092-4d44-9e46-941208920278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e19988-55ba-458e-aae7-e43158de51ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192c9c3-6b3a-4e1e-a37e-ede890ddf7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8fc70-507d-4d0a-a910-c17d73ea7261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f0714-5914-4329-a5a9-45e31df4aa99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d83bc3-e05d-4ba5-b271-083fcc9a1092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863307db-64d9-4eb5-afb1-62f8bc26c684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c0560-ff8b-4bf0-95cb-e0ef485b0a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258e04b-ae0e-47e8-81c7-af9e89903bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#posicoes\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True) \\\n",
    "    .json(f\"s3a://raw/posicoes.csv\")\n",
    "# show data\n",
    "# df.show(10)\n",
    "\n",
    "# df.printSchema()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode_outer,col\n",
    "\n",
    "\n",
    "def flatten(df):\n",
    "   # compute Complex Fields (Lists and Structs) in Schema   \n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df=df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):    \n",
    "         df=df.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "      # recompute remaining Complex Fields in Schema       \n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   return df\n",
    "\n",
    "df_flatten = flatten(df)\n",
    "df_flatten.show()\n",
    "\n",
    "# Write DataFrame to ClickHouse\n",
    "df_flatten.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"dbtable\", \"posicoes\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE=MergeTree() ORDER BY (hr,l_vs_p)\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2cf28-22ba-47fe-a06f-064ac1ddfa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previsao\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True) \\\n",
    "    .json(f\"s3a://raw/previsao.csv\")\n",
    "# show data\n",
    "# df.show(10)\n",
    "\n",
    "# df.printSchema()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode_outer,col\n",
    "\n",
    "\n",
    "def flatten(df):\n",
    "   # compute Complex Fields (Lists and Structs) in Schema   \n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df=df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):    \n",
    "         df=df.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "      # recompute remaining Complex Fields in Schema       \n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   return df\n",
    "\n",
    "df_flatten = flatten(df)\n",
    "df_flatten.show()\n",
    "\n",
    "# Write DataFrame to ClickHouse\n",
    "df_flatten.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"dbtable\", \"previsao\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE=MergeTree() ORDER BY (hr,l_vs_p)\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534844dd-ad06-4f22-837e-546d2f257e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linhas\n",
    "\n",
    "df = spark.read.option(\"inferSchema\", True) \\\n",
    "    .json(f\"s3a://raw/linhas.csv\")\n",
    "# show data\n",
    "# df.show(10)\n",
    "\n",
    "# df.printSchema()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode_outer,col\n",
    "\n",
    "\n",
    "def flatten(df):\n",
    "   # compute Complex Fields (Lists and Structs) in Schema   \n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df=df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):    \n",
    "         df=df.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "      # recompute remaining Complex Fields in Schema       \n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   return df\n",
    "\n",
    "df_flatten = flatten(df)\n",
    "df_flatten.show()\n",
    "\n",
    "# Write DataFrame to ClickHouse\n",
    "df_flatten.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"dbtable\", \"linhas\") \\\n",
    "    .option(\"createTableOptions\", \"ENGINE=MergeTree() ORDER BY (cl)\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
