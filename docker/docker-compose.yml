version: '3.7'
############################
##  DEFAULT
############################
x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@db/airflow
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@db/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@db/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/config:/opt/airflow/config
    - ./airflow/plugins:/opt/airflow/plugins
    - ./util:/util
  user: "${AIRFLOW_UID:-50000}:0"
  
services:
############################
##  DATA LAKE
############################
  minio:
    image: minio/minio:latest
    platform: linux/amd64
    container_name: minio
    entrypoint: sh
    command:   '-c ''mkdir -p /minio_data/raw && mkdir -p /minio_data/trusted && minio server /minio_data --console-address ":9001"'''
    ports:
      - "9050:9000"
      - "9051:9001"
    hostname: minio
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_ACCESS_KEY: datalake
      MINIO_SECRET_KEY: datalake
    volumes:
      - ./minio/data1:/data


  namenode:
    image: fjardim/mds-namenode
    platform: linux/amd64
    container_name: namenode
    hostname: namenode
    volumes:
      - ./hadoop/hdfs/namenode:/hadoop/dfs/name
      - ./hadoop/util:/util
    env_file:
      - ./hadoop/hadoop.env
    ports:
      - "9870:9870"
    deploy:
      resources:
        limits:
          memory: 500m
  
  datanode:
    image: fjardim/mds-datanode
    platform: linux/amd64
    container_name: datanode
    hostname: datanode
    volumes:
      - ./hadoop/hdfs/datanode:/hadoop/dfs/data
      - ./hadoop/util:/util
    env_file:
      - ./hadoop/hadoop.env
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    deploy:
      resources:
        limits:
          memory: 500m


############################
## DATABASE
############################

  presto:
    platform: linux/amd64
    image: prestodb/presto
    hostname: presto
    container_name: presto
    volumes: 
      - ./presto/etc/catalog:/opt/presto-server/etc/catalog/
      - ./presto/etc/catalog:/opt/presto/etc/catalog/
      - ./presto/etc/catalog:/etc/catalog/ 
      - ./presto/etc/hadoop:/hadoop 
      - ./util:/util
    ports:
      - 18080:8080
    depends_on:
      - hive

    deploy:
      resources:
        limits:
          memory: 2g
          #cpus: '0.2'

  adminer:
    image: adminer
    platform: linux/amd64
    container_name: adminer
    hostname: adminer
    ports:
      - 28080:8080
      
 
   hive:
    hostname: hive
    #image: apache/hive:4.0.0-beta-1
    image: fjardim/mds-hive
    platform: linux/amd64
    container_name: hive
    environment:
      AWS_ACCESS_KEY_ID: datalake
      AWS_SECRET_ACCESS_KEY: datalake
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: hiveserver2
      SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083" 
      IS_RESUME: "true"
      #HIVE_VERSION: "3.1.3"
    ports:
       - "10000:10000"
       - "10002:10002"
    depends_on:
      - metastore
    user: root
    volumes:
       - ./hive/conf:/hive_custom_conf
       - ./util:/util
          
  metastore:
    hostname: metastore
    platform: linux/amd64
    image: fjardim/mds-hive-metastore
    #image: apache/hive:4.0.0-beta-1
    container_name: metastore
    environment:
      AWS_ACCESS_KEY_ID: datalake
      AWS_SECRET_ACCESS_KEY: datalake
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: metastore
      #SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083" 
      IS_RESUME: "true"
      DB_DRIVER: postgres 
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://db:5432/metastore -Djavax.jdo.option.ConnectionUserName=admin -Djavax.jdo.option.ConnectionPassword=admin" 
    ports:
       - "9083:9083"
    depends_on:
        - db
          
    user: root
    volumes:
       - ./hive/meta:/opt/hive/data/warehouse 
       - ./hive/conf:/hive_custom_conf
       - ./util:/util

  clickhouse:
    image: clickhouse/clickhouse-server
    platform: linux/amd64
    container_name: clickhouse
    hostname: clickhouse
    ports:
      - "18123:8123"
      - "29000:9000"
    environment:
      CLICKHOUSE_DB: "datalab"
      CLICKHOUSE_USER: "admin"
      CLICKHOUSE_PASSWORD: "admin"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
    volumes:
      #- ./clickhouse/data:/var/lib/clickhouse/ 
      #- ./clickhouse/config/config.xml:/etc/clickhouse-server/config.d/storage.xml
      - ./util:/util

############################
## VISUALIZACAO
############################

  metabase:
      image: metabase/metabase:latest
      platform: linux/amd64
      container_name: metabase
      hostname: metabase
      ports:
        - 3000:3000
      depends_on:
        - db
      environment:
        MB_DB_TYPE: postgres
        MB_DB_DBNAME: metabase
        MB_DB_PORT: 5432
        MB_DB_PASS: admin
        MB_DB_USER: admin
        MB_DB_HOST: db
        MB_PASSWORD_COMPLEXITY: "weak"
        MB_PASSWORD_LENGTH: 4
        #user user@datalab.com.br pass datalab
      volumes:
        - ./util:/util
 
##################################################
### ANALISE E PROCESSAMENTO
##################################################
  spark-master:
    image: fjardim/mds-spark
    platform: linux/amd64
    hostname: spark-master
    container_name: spark-master
    command: 
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-master.sh
        start-notebook.sh --NotebookApp.token=''
    ports:
      - 8889:8888
      - 4040:4040
      - 4041:4041
      - 4042:4042
      - 4043:4043
      - 38080:8080
      - 7077:7077
    volumes:
      - ./spark/work:/home/user 
      - ./spark/env:/env 
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 2g

  spark-worker:
    image: fjardim/mds-spark
    platform: linux/amd64
    hostname: spark-worker
    container_name: spark-worker
    command: 
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-worker.sh spark-master:7077
        start-notebook.sh --NotebookApp.token='' 
    #command: /usr/local/spark/sbin/start-worker.sh jupyter-spark:7077
    #environment:
    #  PYSPARK_SUBMIT_ARGS: "--packages io.delta:delta-core_2.12:2.2.0,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.392"
    env_file:
      - ./spark/env/jupyter.env
    ports:
      - 5040:4040
      - 5041:4041
      - 5042:4042
      - 5043:4043
      - 38081:8081
      - 36533:36533
    volumes:
      - ./util:/util
      - ./spark/work:/home/user 
    environment:
      SPARK_MASTER: spark-master
    depends_on:
        - spark-master
    deploy:
      resources:
        limits:
          memory: 1g

  datahub-actions:
    depends_on:
      datahub-gms:
        condition: service_healthy
    environment:
      - ACTIONS_CONFIG=${ACTIONS_CONFIG:-}
      - ACTIONS_EXTRA_PACKAGES=${ACTIONS_EXTRA_PACKAGES:-}
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_GMS_PROTOCOL=http
      - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
      - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
      - KAFKA_PROPERTIES_SECURITY_PROTOCOL=PLAINTEXT
      - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
      - METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME=MetadataChangeLog_Versioned_v1
      - SCHEMA_REGISTRY_URL=http://kafka-schema-registry:8081
    hostname: actions
    image: ${DATAHUB_ACTIONS_IMAGE:-acryldata/datahub-actions}:${ACTIONS_VERSION:-head}
    container_name: datahub-actions

  datahub-frontend-react:
    depends_on:
      datahub-gms:
        condition: service_healthy
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_SECRET=YouKnowNothing
      - DATAHUB_APP_VERSION=1.0
      - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
      - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml -Dlogback.debug=false -Dpidfile.path=/dev/null
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
      - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
      - ELASTIC_CLIENT_HOST=elasticsearch
      - ELASTIC_CLIENT_PORT=9200
    hostname: datahub-frontend-react
    image: ${DATAHUB_FRONTEND_IMAGE:-linkedin/datahub-frontend-react}:${DATAHUB_VERSION:-head}
    ports:
      - 9002:9002
    volumes:
      - ./datahub/plugins:/etc/datahub/plugins
    container_name: datahub-frontend

  datahub-gms:
    depends_on:
      datahub-upgrade:
        condition: service_completed_successfully
    environment:
      - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-quickstart}
      - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_PASSWORD=root
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_USERNAME=root
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_PORT=9200
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - ENTITY_SERVICE_ENABLE_RETENTION=true
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - GRAPH_SERVICE_DIFF_MODE_ENABLED=true
      - GRAPH_SERVICE_IMPL=elasticsearch
      - JAVA_OPTS=-Xms1g -Xmx1g
      - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
      - KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR=${KAFKA_CONSUMER_STOP_ON_DESERIALIZATION_ERROR:-true}
      - KAFKA_SCHEMAREGISTRY_URL=http://kafka-schema-registry:8081
      - MAE_CONSUMER_ENABLED=true
      - MCE_CONSUMER_ENABLED=true
      - PE_CONSUMER_ENABLED=true
      - UI_INGESTION_ENABLED=true
    healthcheck:
      interval: 1s
      retries: 3
      start_period: 90s
      test: curl -sS --fail http://datahub-gms:${DATAHUB_GMS_PORT:-8080}/health
      timeout: 5s
    hostname: datahub-gms
    image: ${DATAHUB_GMS_IMAGE:-linkedin/datahub-gms}:${DATAHUB_VERSION:-head}
    ports:
    - 8080:8080
    volumes:
    - ./datahub/plugins:/etc/datahub/plugins
    container_name: datahub-gms

  datahub-upgrade:
    command:
    - -u
    - SystemUpdate
    depends_on:
      elasticsearch-setup:
        condition: service_completed_successfully
      kafka-setup:
        condition: service_completed_successfully
      mysql-setup:
        condition: service_completed_successfully
    environment:
    - EBEAN_DATASOURCE_USERNAME=root
    - EBEAN_DATASOURCE_PASSWORD=root
    - EBEAN_DATASOURCE_HOST=mysql:3306
    - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
    - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
    - KAFKA_BOOTSTRAP_SERVER=kafka-broker:9092
    - KAFKA_SCHEMAREGISTRY_URL=http://kafka-schema-registry:8081
    - ELASTICSEARCH_HOST=elasticsearch
    - ELASTICSEARCH_PORT=9200
    - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
    - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
    - ELASTICSEARCH_BUILD_INDICES_CLONE_INDICES=false
    - GRAPH_SERVICE_IMPL=elasticsearch
    - DATAHUB_GMS_HOST=datahub-gms
    - DATAHUB_GMS_PORT=8080
    - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
    - BACKFILL_BROWSE_PATHS_V2=true
    - REPROCESS_DEFAULT_BROWSE_PATHS_V2=false
    hostname: datahub-upgrade
    image: ${DATAHUB_UPGRADE_IMAGE:-acryldata/datahub-upgrade}:${DATAHUB_VERSION:-head}
    container_name: datahub-upgrade

  pgadmin:
    container_name: pgadmin_container
    image: dpage/pgadmin4
    platform: linux/amd64
    environment:
      PGADMIN_DEFAULT_EMAIL: lab-pgadmin4@pgadmin.org
      PGADMIN_DEFAULT_PASSWORD: admin    
    ports:
      - "5433:80"
    depends_on:
      - db  

#############################
## GENERAL

configs:
  flags:
    file: ./airbyte/flags.yml

networks:
  datalab:
    driver: bridge
